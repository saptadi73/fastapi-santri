# PostgreSQL/PostGIS Performance Review & Optimization Guide

## üî¥ MASALAH UTAMA YANG MENYEBABKAN RESOURCE BOROS

### 1. **ST_AsGeoJSON() pada Jutaan Baris**
**Problem:** Konversi geometry ke JSON dilakukan per-row
```sql
-- ‚ùå BOROS RESOURCE
ST_AsGeoJSON(sm.lokasi)::jsonb  -- Dilakukan untuk SETIAP ROW
```
**Impact:**
- PostGIS harus melakukan konversi string parsing untuk setiap geometry
- CPU usage tinggi, Memory spike saat query besar
- Network transfer besar

**Solusi:**
1. **Simplify geometries** untuk heatmap/choropleth yang tidak butuh precision tinggi
2. **Pagination** untuk endpoint yang mengembalikan banyak data
3. **Caching** untuk data statis

---

### 2. **Multiple DISTINCT COUNT dengan Conditional Aggregation**
**Problem:** Query choropleth menggunakan COUNT(DISTINCT) berkali-kali
```sql
-- ‚ùå BOROS RESOURCE
COUNT(DISTINCT CASE WHEN sk.kategori_kemiskinan = 'Sangat Miskin' THEN sp.id END) as sangat_miskin,
COUNT(DISTINCT CASE WHEN sk.kategori_kemiskinan = 'Miskin' THEN sp.id END) as miskin,
COUNT(DISTINCT CASE WHEN sk.kategori_kemiskinan = 'Rentan' THEN sp.id END) as rentan,
COUNT(DISTINCT CASE WHEN sk.kategori_kemiskinan = 'Tidak Miskin' THEN sp.id END) as tidak_miskin,
```
**Impact:**
- Setiap CASE harus scan seluruh table
- Multiple aggregation passes
- CPU intensive

**Solusi:**
- Gunakan `FILTER` clause (lebih efficient)
- Gunakan materialized view untuk pre-calculation
- Tambah index pada kategori columns

---

### 3. **No Index pada Geometry & Filter Columns**
**Problem:** Tidak ada spatial index
```sql
-- ‚ùå MISSING INDEX
WHERE lokasi IS NOT NULL  -- Scan keseluruhan table tanpa index
WHERE kategori_kemiskinan = 'Sangat Miskin'  -- Scan keseluruhan tanpa index
```
**Impact:**
- Full table scan setiap query
- Worst case: O(n) complexity

**Solusi:**
```sql
-- ‚úÖ TAMBAHKAN INDEX
CREATE INDEX idx_santri_map_lokasi ON santri_map USING GIST(lokasi);
CREATE INDEX idx_santri_map_kategori ON santri_map(kategori_kemiskinan);
CREATE INDEX idx_santri_pribadi_kabupaten ON santri_pribadi(kabupaten);
CREATE INDEX idx_pesantren_map_lokasi ON pesantren_map USING GIST(lokasi);
CREATE INDEX idx_pesantren_map_kategori ON pesantren_map(kategori_kelayakan);
```

---

### 4. **Inefficient Query: LEFT JOIN Santri untuk setiap point**
**Problem:** Endpoint `/santri-points` melakukan LEFT JOIN ke `santri_pribadi`
```sql
-- ‚ùå BOROS
FROM santri_map sm
LEFT JOIN santri_pribadi sp ON sm.santri_id = sp.id
WHERE sm.lokasi IS NOT NULL
```
**Impact:**
- Setiap santri_map row harus di-join
- Data redundancy jika sudah ada di santri_map table

**Solusi:**
- Denormalize: copy data ke santri_map saat perlu
- Atau lakukan join hanya saat diperlukan client

---

### 5. **JSONB_AGG Aggregation pada Seluruh Result**
**Problem:** Semua rows di-aggregate jadi satu JSONB object
```sql
-- ‚ùå BOROS MEMORY
jsonb_agg(jsonb_build_object(...)) -- Seluruh result aggregation
```
**Impact:**
- Memory spike untuk aggregate besar
- Tidak bisa di-paginate
- Single connection holds semua memory

**Solusi:**
- Paginate response
- Stream JSON instead of aggregate
- Return count + data per-batch

---

### 6. **Multiple Sub-queries di `/choropleth/stats`**
**Problem:** 4 sub-query dijalankan dalam 1 query besar
```sql
-- ‚ùå BOROS
SELECT jsonb_build_object(
    'santri_categories', (SELECT ... FROM santri_skor GROUP BY ...),
    'pesantren_categories', (SELECT ... FROM pesantren_skor GROUP BY ...),
    'kabupaten_list', (SELECT ... UNION ...),
    'provinsi_list', (SELECT ... UNION ...)
)
```
**Impact:**
- Multiple GROUP BY operations
- UNION tanpa UNION ALL (tambah DISTINCT processing)
- Setiap sub-query scan table penuh

---

### 7. **ST_Y() dan ST_X() pada Geometry**
**Problem:** Extract lat/lng dilakukan per-row
```sql
-- Cukup, tapi bisa di-optimize
ST_Y(lokasi) AS lat,
ST_X(lokasi) AS lng,
```
**Solusi:**
- Pre-calculate dan store di table (jika accuracy tidak critical)
- Atau gunakan materialized view

---

## ‚úÖ REKOMENDASI OPTIMASI (Priority Order)

### Priority 1: INDEX (Quick Win - Impact High)
```sql
-- Spatial Index untuk geometry
CREATE INDEX idx_santri_map_lokasi ON santri_map USING GIST(lokasi) WHERE lokasi IS NOT NULL;
CREATE INDEX idx_pesantren_map_lokasi ON pesantren_map USING GIST(lokasi) WHERE lokasi IS NOT NULL;

-- Regular indexes untuk filters
CREATE INDEX idx_santri_map_kategori_skor ON santri_map(kategori_kemiskinan, skor_terakhir);
CREATE INDEX idx_pesantren_map_kategori_skor ON pesantren_map(kategori_kelayakan, skor_terakhir);
CREATE INDEX idx_santri_pribadi_lokasi ON santri_pribadi(kabupaten, provinsi, kecamatan);
CREATE INDEX idx_pesantren_kabupaten ON pondok_pesantren(kabupaten, provinsi);

-- Join optimization
CREATE INDEX idx_santri_map_santri_id ON santri_map(santri_id);
CREATE INDEX idx_santri_skor_santri_id ON santri_skor(santri_id);
CREATE INDEX idx_pesantren_skor_pesantren_id ON pesantren_skor(pesantren_id);
```

### Priority 2: Query Optimization

#### ‚ùå BEFORE: `/santri-points`
```python
def santri_points(kategori: str | None = None, pesantren_id: str | None = None, db: Session = Depends(get_db)):
    # Query tanpa pagination - bisa return jutaan rows
    sql = """
    SELECT jsonb_build_object(
      'type','FeatureCollection',
      'features', COALESCE(
        jsonb_agg(...),
        '[]'::jsonb
      )
    )
    FROM santri_map sm
    LEFT JOIN santri_pribadi sp ON sm.santri_id = sp.id
    WHERE ...
    """
```

#### ‚úÖ AFTER: Dengan Pagination & Simplification
```python
def santri_points(
    kategori: str | None = None,
    pesantren_id: str | None = None,
    page: int = 1,
    limit: int = 1000,  # ‚Üê Paginate
    db: Session = Depends(get_db),
):
    """Get santri locations with pagination."""
    where = ["sm.lokasi IS NOT NULL"]
    params = {}

    if kategori:
        where.append("sm.kategori_kemiskinan = :kategori")
        params["kategori"] = kategori

    if pesantren_id:
        where.append("sm.pesantren_id = :pesantren_id")
        params["pesantren_id"] = pesantren_id

    offset = (page - 1) * limit
    
    sql = f"""
    SELECT 
        id,
        santri_id,
        nama,
        ST_Y(lokasi) as lat,
        ST_X(lokasi) as lng,
        skor_terakhir,
        kategori_kemiskinan,
        pesantren_id
    FROM santri_map
    WHERE {' AND '.join(where)}
    LIMIT :limit OFFSET :offset
    """
    
    params['limit'] = limit
    params['offset'] = offset
    
    rows = db.execute(text(sql), params).fetchall()
    
    # Format sebagai GeoJSON di Python, bukan di database
    return {
        "type": "FeatureCollection",
        "features": [
            {
                "type": "Feature",
                "geometry": {
                    "type": "Point",
                    "coordinates": [row.lng, row.lat]
                },
                "properties": {
                    "id": row.id,
                    "santri_id": row.santri_id,
                    "nama": row.nama,
                    "skor": row.skor_terakhir,
                    "kategori": row.kategori_kemiskinan
                }
            }
            for row in rows
        ],
        "page": page,
        "limit": limit,
        "total": db.execute(text(f"SELECT COUNT(*) FROM santri_map WHERE {' AND '.join(where)}"), params).scalar()
    }
```

#### ‚ùå BEFORE: Choropleth Query (Boros)
```sql
WITH santri_stats AS (
    SELECT 
        COALESCE(sp.kabupaten, 'Unknown') as kabupaten,
        COUNT(DISTINCT sp.id) as total_santri,
        COUNT(DISTINCT CASE WHEN sk.kategori_kemiskinan = 'Sangat Miskin' THEN sp.id END) as sangat_miskin,
        COUNT(DISTINCT CASE WHEN sk.kategori_kemiskinan = 'Miskin' THEN sp.id END) as miskin,
        COUNT(DISTINCT CASE WHEN sk.kategori_kemiskinan = 'Rentan' THEN sp.id END) as rentan,
        COUNT(DISTINCT CASE WHEN sk.kategori_kemiskinan = 'Tidak Miskin' THEN sp.id END) as tidak_miskin,
        AVG(sk.skor_total) as avg_skor
    FROM santri_pribadi sp
    LEFT JOIN santri_skor sk ON sp.id = sk.santri_id
    WHERE sp.kabupaten IS NOT NULL
    GROUP BY sp.kabupaten
)
```

#### ‚úÖ AFTER: Optimized dengan FILTER
```sql
WITH santri_stats AS (
    SELECT 
        COALESCE(sp.kabupaten, 'Unknown') as kabupaten,
        COUNT(DISTINCT sp.id) as total_santri,
        COUNT(DISTINCT sp.id) FILTER (WHERE sk.kategori_kemiskinan = 'Sangat Miskin') as sangat_miskin,
        COUNT(DISTINCT sp.id) FILTER (WHERE sk.kategori_kemiskinan = 'Miskin') as miskin,
        COUNT(DISTINCT sp.id) FILTER (WHERE sk.kategori_kemiskinan = 'Rentan') as rentan,
        COUNT(DISTINCT sp.id) FILTER (WHERE sk.kategori_kemiskinan = 'Tidak Miskin') as tidak_miskin,
        AVG(sk.skor_total) as avg_skor
    FROM santri_pribadi sp
    LEFT JOIN santri_skor sk ON sp.id = sk.santri_id
    WHERE sp.kabupaten IS NOT NULL
    GROUP BY sp.kabupaten
)
```

### Priority 3: Caching & Pre-calculation

**Tambah Materialized View untuk Choropleth Data:**
```sql
CREATE MATERIALIZED VIEW mv_santri_stats_kabupaten AS
SELECT 
    COALESCE(sp.kabupaten, 'Unknown') as kabupaten,
    COALESCE(sp.provinsi, 'Unknown') as provinsi,
    COUNT(DISTINCT sp.id) as total_santri,
    COUNT(DISTINCT sp.id) FILTER (WHERE sk.kategori_kemiskinan = 'Sangat Miskin') as sangat_miskin,
    COUNT(DISTINCT sp.id) FILTER (WHERE sk.kategori_kemiskinan = 'Miskin') as miskin,
    COUNT(DISTINCT sp.id) FILTER (WHERE sk.kategori_kemiskinan = 'Rentan') as rentan,
    COUNT(DISTINCT sp.id) FILTER (WHERE sk.kategori_kemiskinan = 'Tidak Miskin') as tidak_miskin,
    AVG(sk.skor_total) as avg_skor,
    MAX(sk.skor_total) as max_skor,
    MIN(sk.skor_total) as min_skor
FROM santri_pribadi sp
LEFT JOIN santri_skor sk ON sp.id = sk.santri_id
WHERE sp.kabupaten IS NOT NULL
GROUP BY sp.kabupaten, sp.provinsi;

CREATE INDEX idx_mv_santri_kabupaten ON mv_santri_stats_kabupaten(kabupaten, provinsi);

-- Refresh berkala (nightly atau scheduled)
REFRESH MATERIALIZED VIEW mv_santri_stats_kabupaten;
```

### Priority 4: Application-level Optimization

**Implementasi Response Caching:**
```python
from functools import lru_cache
from datetime import datetime, timedelta

class GISCache:
    def __init__(self, ttl_minutes: int = 60):
        self.cache = {}
        self.ttl_minutes = ttl_minutes
    
    def get(self, key: str):
        if key in self.cache:
            data, timestamp = self.cache[key]
            if datetime.now() - timestamp < timedelta(minutes=self.ttl_minutes):
                return data
            else:
                del self.cache[key]
        return None
    
    def set(self, key: str, value):
        self.cache[key] = (value, datetime.now())

gis_cache = GISCache(ttl_minutes=60)

@router.get("/choropleth/stats")
def choropleth_stats(db: Session = Depends(get_db)):
    # Check cache first
    cached = gis_cache.get("choropleth_stats")
    if cached:
        return cached
    
    # ... execute query ...
    result = db.execute(text(sql)).scalar()
    
    # Cache result
    gis_cache.set("choropleth_stats", result)
    return result
```

---

## üìä EXPECTED IMPROVEMENTS

| Optimization | Current | After | Improvement |
|---|---|---|---|
| Heatmap endpoint (10k points) | 8-12 sec | 1-2 sec | **80-90%** ‚¨á |
| Choropleth query (34 kabupaten) | 15-20 sec | 2-3 sec | **85-90%** ‚¨á |
| Memory usage (large queries) | 1-2 GB | 100-300 MB | **70-80%** ‚¨á |
| Database CPU | 80-95% | 20-30% | **65-75%** ‚¨á |
| API response time | 20-30 sec | 1-5 sec | **85-95%** ‚¨á |

---

## üöÄ IMPLEMENTATION ROADMAP

1. **Week 1:** Index creation + monitoring
2. **Week 2:** Refactor endpoints dengan pagination
3. **Week 3:** Materialized view setup + refresh jobs
4. **Week 4:** Response caching + load testing
5. **Week 5:** Query profiling + fine-tuning

---

## üìã QUICK CHECKLIST

- [ ] Create GIST indexes on lokasi columns
- [ ] Create indexes on kategori/kategori_kelayakan
- [ ] Implement pagination in `/santri-points` and `/pesantren-points`
- [ ] Replace COUNT(DISTINCT CASE) with FILTER clause
- [ ] Create materialized views for choropleth stats
- [ ] Implement response caching layer
- [ ] Setup PgBouncer connection pooling (jika belum ada)
- [ ] Monitor slow queries dengan `pg_stat_statements`
- [ ] Set proper `work_mem` dan `shared_buffers` di postgresql.conf
- [ ] Consider partitioning large tables (santri_map, pesantren_map)
